# vim: tabstop=4 shiftwidth=4 softtabstop=4

# Copyright 2013 Violin Memory, Inc.
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

"""
Violin Memory iSCSI Driver for Openstack Cinder

Uses Violin REST API via XG-Tools to manage a standard V6000 series
flash array to provide network block-storage services.

by Ryan Lucio
Senior Software Engineer
Violin Memory

---

Driver support (verified for G5.5.2):
-------------------------------------
Driver Setup:                   YES
Volume Create/Delete:           YES
Export Create/Remove:           YES
Volume Attach/Detach:           YES
Snapshot Create/Delete:         NO
Create Volume from Snapshot:    NO
Clone Volume:                   YES
Get Volume Stats:               YES
Volume Extend:                  YES
Volume Migrate:                 NO*
Copy Image to Volume:           YES*
Copy Volume to Image:           YES*

* functionality inherited from base class driver
"""

import random
import time

from oslo.config import cfg

from cinder.db.sqlalchemy import models
from cinder import exception
from cinder.openstack.common import log as logging
from cinder import utils
from cinder.volume.drivers.violin import v6000_common

LOG = logging.getLogger(__name__)

try:
    from . import version
    __version__ = version.__version__
except Exception:
    # version.py is autogenerated during packaging. If we are running
    # against original source it will not be present.
    __version__ = "unknown"

violin_extra_opts = [
    cfg.IntOpt('gateway_iscsi_port',
               default=3260,
               help='IP port to use for iSCSI targets'),
    cfg.StrOpt('gateway_iscsi_target_prefix',
               default='iqn.2004-02.com.vmem:',
               help='prefix for iscsi volumes'), ]

CONF = cfg.CONF
CONF.register_opts(violin_extra_opts)


class V6000ISCSIDriver(v6000_common.V6000CommonDriver):
    """Executes commands relating to Violin Memory Arrays."""

    def __init__(self, *args, **kwargs):
        super(V6000ISCSIDriver, self).__init__(*args, **kwargs)
        self.array_info = []
        self.gateway_iscsi_ip_addresses_mga = []
        self.gateway_iscsi_ip_addresses_mgb = []
        self.config = kwargs.get('configuration', None)
        self.context = None
        if self.config:
            self.config.append_config_values(violin_extra_opts)

        LOG.info(_("Initialized driver %(name)s version: %(vers)s") %
                 {'name': self.__class__.__name__, 'vers': __version__})

    def do_setup(self, context):
        """Any initialization the driver does while starting."""
        super(V6000ISCSIDriver, self).do_setup(context)
        self.gateway_iscsi_ip_addresses_mga = self._get_active_iscsi_ips(
            self.vmem_mga)
        for ip in self.gateway_iscsi_ip_addresses_mga:
            self.array_info.append({"node": self._get_hostname('mga'),
                                    "addr": ip,
                                    "conn": self.vmem_mga})
        self.gateway_iscsi_ip_addresses_mgb = self._get_active_iscsi_ips(
            self.vmem_mgb)
        for ip in self.gateway_iscsi_ip_addresses_mgb:
            self.array_info.append({"node": self._get_hostname('mgb'),
                                    "addr": ip,
                                    "conn": self.vmem_mgb})

    def check_for_setup_error(self):
        """Returns an error if prerequisites aren't met."""
        super(V6000ISCSIDriver, self).check_for_setup_error()
        vip = self.vmem_vip.basic
        bn = "/vshare/config/iscsi/enable"
        resp = vip.get_node_values(bn)
        if resp[bn] != True:
            raise v6000_common.InvalidBackendConfig(
                reason=_('iSCSI is not enabled'))
        if len(self.gateway_iscsi_ip_addresses_mga) == 0:
            raise v6000_common.InvalidBackendConfig(
                reason=_('no available iSCSI IPs on mga'))
        if len(self.gateway_iscsi_ip_addresses_mgb) == 0:
            raise v6000_common.InvalidBackendConfig(
                reason=_('no available iSCSI IPs on mgb'))

    def ensure_export(self, context, volume):
        """Synchronously checks and re-exports volumes at cinder start time."""
        pass

    def create_export(self, context, volume):
        """Exports the volume."""
        pass

    def remove_export(self, context, volume):
        """Removes an export for a logical volume."""
        pass

    def initialize_connection(self, volume, connector):
        """Initializes the connection (target<-->initiator)."""
        igroup = None

        if self.config.use_igroups:
            #
            # Most drivers don't use igroups, because there are a
            # number of issues with multipathing and iscsi/fcp where
            # lun devices either aren't cleaned up properly or are
            # stale (from previous scans).
            #

            # If the customer really wants igroups for whatever
            # reason, we create a new igroup for each host/hypervisor.
            # Every lun that is exported to the particular
            # hypervisor/host will be contained in this igroup.  This
            # should prevent other hosts from seeing luns they aren't
            # using when they perform scans.
            #
            igroup = self._get_igroup(volume, connector)
            self._add_igroup_member(connector, igroup)

        vol = self._get_short_name(volume['id'])
        tgt = self._create_iscsi_target(volume)
        if isinstance(volume, models.Volume):
            lun = self._export_lun(volume, connector, igroup)
        else:
            lun = self._export_snapshot(volume, connector, igroup)

        iqn = "%s%s:%s" % (self.config.gateway_iscsi_target_prefix,
                           tgt['node'], vol)
        self.vmem_vip.basic.save_config()

        properties = {}
        properties['target_discovered'] = False
        properties['target_portal'] = '%s:%s' % (tgt['addr'], '3260')
        properties['target_iqn'] = iqn
        properties['target_lun'] = lun
        properties['volume_id'] = volume['id']
        properties['auth_method'] = 'CHAP'
        properties['auth_username'] = ''
        properties['auth_password'] = ''

        return {'driver_volume_type': 'iscsi', 'data': properties}

    def terminate_connection(self, volume, connector, force=False, **kwargs):
        """Terminates the connection (target<-->initiator)."""
        if isinstance(volume, models.Volume):
            self._unexport_lun(volume)
        else:
            self._unexport_snapshot(volume)
        self._delete_iscsi_target(volume)
        self.vmem_vip.basic.save_config()

    def get_volume_stats(self, refresh=False):
        """Get volume stats."""
        if refresh or not self.stats:
            self._update_stats()
        return self.stats

    @utils.synchronized('vmem-export')
    def _create_iscsi_target(self, volume):
        """Creates a new target for use in exporting a lun

        Openstack does not yet support multipathing. We still create
        HA targets but we pick a single random target for the
        Openstack infrastructure to use.  This at least allows us to
        evenly distribute LUN connections across the storage cluster.
        The equivalent CLI commands are "iscsi target create
        <target_name>" and "iscsi target bind <target_name> to
        <ip_of_mg_eth_intf>".

        Arguments:
            volume -- volume object provided by the Manager

        Returns:
            reference to randomly selected target object
        """
        v = self.vmem_vip
        target_name = self._get_short_name(volume['id'])

        LOG.info(_("Creating iscsi target %s"), target_name)

        try:
            self._send_cmd_and_verify(v.iscsi.create_iscsi_target,
                                      self._wait_for_targetstate,
                                      '', [target_name], [target_name])

        except Exception:
            LOG.exception(_("Failed to create iscsi target!"))
            raise

        try:
            self._send_cmd(self.vmem_mga.iscsi.bind_ip_to_target, '',
                           target_name, self.gateway_iscsi_ip_addresses_mga)
            self._send_cmd(self.vmem_mgb.iscsi.bind_ip_to_target, '',
                           target_name, self.gateway_iscsi_ip_addresses_mgb)
        except Exception:
            LOG.exception(_("Failed to bind iSCSI targets!"))
            raise

        return self.array_info[random.randint(0, len(self.array_info) - 1)]

    @utils.synchronized('vmem-export')
    def _delete_iscsi_target(self, volume):
        """Deletes the iscsi target for a lun

        The CLI equivalent is "no iscsi target create <target_name>".

        Arguments:
            volume -- volume object provided by the Manager
        """
        v = self.vmem_vip
        target_name = self._get_short_name(volume['id'])

        LOG.info(_("Deleting iscsi target for %s"), target_name)

        try:
            self._send_cmd(v.iscsi.delete_iscsi_target, '', target_name)
        except Exception:
            LOG.exception(_("Failed to delete iSCSI target!"))
            raise

    @utils.synchronized('vmem-export')
    def _export_lun(self, volume, connector=None, igroup=None):
        """Generates the export configuration for the given volume

        The equivalent CLI command is "lun export container
        <container_name> name <lun_name>"

        Arguments:
            volume -- volume object provided by the Manager
            connector -- connector object provided by the Manager
            igroup -- name of igroup to use for exporting

        Returns:
            lun_id -- the LUN ID assigned by the backend
        """
        lun_id = ''
        export_to = ''
        v = self.vmem_vip

        if igroup:
            export_to = igroup
        elif connector:
            export_to = connector['initiator']
        else:
            raise exception.Error(_("No initiators found, cannot proceed"))

        lun_id = self.lun_tracker.get_lun_id_for_volume(volume)

        target_name = self._get_short_name(volume['id'])

        LOG.info(_("Exporting lun %(vol_id)s on lun_id %(lun_id)s") %
                 {'vol_id': volume['id'], 'lun_id': lun_id})

        try:
            self._send_cmd_and_verify(v.lun.export_lun,
                                      self._wait_for_exportstate,
                                      '',
                                      [self.container, volume['id'],
                                      target_name, export_to, lun_id],
                                      [volume['id'], True])

        except Exception:
            LOG.exception(_("LUN export failed!"))
            raise

        return lun_id

    @utils.synchronized('vmem-export')
    def _unexport_lun(self, volume):
        """Removes the export configuration for the given volume.

        The equivalent CLI command is "no lun export container
        <container_name> name <lun_name>"

        Arguments:
            volume -- volume object provided by the Manager
        """
        v = self.vmem_vip

        LOG.info(_("Unexporting lun %s"), volume['id'])

        try:
            self._send_cmd_and_verify(v.lun.unexport_lun,
                                      self._wait_for_exportstate,
                                      '',
                                      [self.container, volume['id'],
                                      'all', 'all', 'auto'],
                                      [volume['id'], False])

        except v6000_common.ViolinBackendErrNotFound:
            LOG.info(_("Lun %s already unexported, continuing"),
                     volume['id'])

        except Exception:
            LOG.exception(_("LUN unexport failed!"))
            raise

    @utils.synchronized('vmem-export')
    def _export_snapshot(self, snapshot, connector=None, igroup=None):
        """Generates the export configuration for the given snapshot.

        The equivalent CLI command is "snapshot export container
        PROD08 lun <snapshot_name> name <volume_name>"

        Arguments:
            snapshot -- snapshot object provided by the Manager
            connector -- connector object provided by the Manager
            igroup -- name of igroup to use for exporting

        Returns:
            lun_id -- the LUN ID assigned by the backend
        """
        lun_id = ''
        export_to = ''
        v = self.vmem_vip

        target_name = self._get_short_name(snapshot['id'])

        LOG.info(_("Exporting snapshot %s"), snapshot['id'])

        lun_id = self.lun_tracker.get_lun_id_for_snapshot(snapshot)

        if igroup:
            export_to = igroup
        elif connector:
            export_to = connector['initiator']
        else:
            raise exception.Error(_("No initiators found, cannot proceed"))

        try:
            self._send_cmd(v.snapshot.export_lun_snapshot, '',
                           self.container, snapshot['volume_id'],
                           snapshot['id'], export_to, target_name, lun_id)

        except Exception:
            LOG.exception(_("Snapshot export failed!"))
            raise

        else:
            self._wait_for_exportstate(snapshot['id'], True)

        return lun_id

    @utils.synchronized('vmem-export')
    def _unexport_snapshot(self, snapshot):
        """Removes the export configuration for the given snapshot.

        The equivalent CLI command is "no snapshot export container
        PROD08 lun <snapshot_name> name <volume_name>"

        Arguments:
            snapshot -- snapshot object provided by the Manager
        """
        v = self.vmem_vip

        LOG.info(_("Unexporting snapshot %s"), snapshot['id'])

        try:
            self._send_cmd(v.snapshot.unexport_lun_snapshot, '',
                           self.container, snapshot['volume_id'],
                           snapshot['id'], 'all', 'all', 'auto', False)

        except Exception:
            LOG.exception(_("Snapshot export failed!"))
            raise

        else:
            self._wait_for_exportstate(snapshot['id'], False)

    def _add_igroup_member(self, connector, igroup):
        """Add an initiator to the openstack igroup so it can see exports.

        The equivalent CLI command is "igroup addto name <igroup_name>
        initiators <initiator_name>"

        Arguments:
            connector -- connector object provided by the Manager
        """
        v = self.vmem_vip

        LOG.info(_("Adding initiator %s to igroup"), connector['initiator'])

        resp = v.igroup.add_initiators(igroup, connector['initiator'])

        if resp['code'] != 0:
            raise exception.Error(
                _('Failed to add igroup member: %(code)d, %(message)s') % resp)

    def _update_stats(self):
        """Gathers array stats from the backend and converts them to GB values.
        """
        data = {}
        total_gb = 'unknown'
        free_gb = 'unknown'
        v = self.vmem_vip

        master_cluster_id = self.vmem_vip.basic.get_node_values(
            '/cluster/state/master_id').values()[0]

        bn1 = "/vshare/state/global/%s/container/%s/total_bytes" \
            % (master_cluster_id, self.container)
        bn2 = "/vshare/state/global/%s/container/%s/free_bytes" \
            % (master_cluster_id, self.container)
        resp = v.basic.get_node_values([bn1, bn2])

        if bn1 in resp:
            total_gb = resp[bn1] / 1024 / 1024 / 1024
        else:
            LOG.warn(_("Failed to receive update for total_gb stat!"))

        if bn2 in resp:
            free_gb = resp[bn2] / 1024 / 1024 / 1024
        else:
            LOG.warn(_("Failed to receive update for free_gb stat!"))

        backend_name = self.config.volume_backend_name
        data['volume_backend_name'] = backend_name or self.__class__.__name__
        data['vendor_name'] = 'Violin Memory, Inc.'
        data['driver_version'] = __version__
        data['storage_protocol'] = 'iSCSI'
        data['reserved_percentage'] = 0
        data['QoS_support'] = False
        data['total_capacity_gb'] = total_gb
        data['free_capacity_gb'] = free_gb

        for i in data:
            LOG.debug(_("stat update: %(name)s=%(data)s") %
                      {'name': i, 'data': data[i]})

        self.stats = data

    def _get_short_name(self, volume_name):
        """Creates a vSHARE-compatible iSCSI target name.

        The Folsom-style volume names are prefix(7) + uuid(36), which
        is too long for vSHARE for target names.  To keep things
        simple we can just truncate the name to 32 chars.

        Arguments:
            volume_name -- name of volume/lun

        Returns:
            Shortened volume name as a string.
        """
        return volume_name[:32]

    def _get_active_iscsi_ips(self, mg_conn):
        """Get a list of gateway IP addresses that can be used for iSCSI.

        Arguments:
            mg_conn -- active XG connection to one of the gateways

        Returns:
            active_gw_iscsi_ips -- list of IP addresses
        """
        active_gw_iscsi_ips = []
        interfaces_to_skip = ['lo', 'vlan10', 'eth1', 'eth2', 'eth3']

        bn = "/net/interface/config/*"
        intf_list = mg_conn.basic.get_node_values(bn)

        for i in intf_list:
            if intf_list[i] in interfaces_to_skip:
                continue

            bn1 = "/net/interface/state/%s/addr/ipv4/1/ip" % intf_list[i]
            bn2 = "/net/interface/state/%s/flags/link_up" % intf_list[i]
            resp = mg_conn.basic.get_node_values([bn1, bn2])

            if len(resp.keys()) == 2 and resp[bn2] == True:
                active_gw_iscsi_ips.append(resp[bn1])

        return active_gw_iscsi_ips

    def _get_hostname(self, mg_to_query=None):
        """Get the hostname of one of the mgs (hostname is used in IQN).
        If the remote query fails then fall back to using the hostname
        provided in the cinder configuration file.

        Arguments:
            mg_to_query -- name of gateway to query 'mga' or 'mgb'

        Returns: hostname -- hostname as a string
        """
        hostname = self.config.gateway_vip
        conn = self.vmem_vip.basic

        if mg_to_query == "mga":
            hostname = self.config.gateway_mga
            conn = self.vmem_mga.basic
        elif mg_to_query == "mgb":
            hostname = self.config.gateway_mgb
            conn = self.vmem_mgb.basic

        ret_dict = conn.get_node_values("/system/hostname")
        if ret_dict:
            hostname = ret_dict.items()[0][1]
        else:
            LOG.debug(_("Unable to fetch gateway hostname for %s"),
                      mg_to_query)

        return hostname

    def _get_lun_id(self, volume_name):
        """Queries the gateway to find the lun id for the exported
        volume.  Technically a lun id is assigned for each target, but
        it is the same value for all targets.

        Arguments:
            volume_name    -- LUN to query

        Returns:
            LUN ID for the exported lun as an integer.  If no LUN ID
            is found, return -1.
        """
        vip = self.vmem_vip.basic
        lun_id = -1

        prefix = "/vshare/config/export/container"
        bn = "%s/%s/lun/%s/target/**" \
            % (prefix, self.container, volume_name)
        resp = vip.get_node_values(bn)

        # EX: /vshare/config/export/container/PROD08/lun/test1/target/hba-b2/
        #     initiator/openstack/lun_id = 1 (int16)
        #
        for node in resp:
            if node.endswith('/lun_id'):
                lun_id = resp[node]
                break

        # TODO(rdl): add exception for case where no lun id found, or lun ids
        # do not match
        #
        return lun_id

    def _wait_for_targetstate(self, target_name):
        """Polls backend to verify an iscsi target configuration.

        This function will try to verify the creation or removal of
        an iscsi target on both gateway nodes of the array every 5
        seconds for up to 30 seconds.

        Arguments:
            target_name -- name of iscsi target to be polled

        Returns:
            True if the export state was correctly added
        """
        status = [False, False]
        mg_conns = [self.vmem_mga.basic, self.vmem_mgb.basic]
        success = False

        bn = "/vshare/config/iscsi/target/%s" % (target_name)

        for i in xrange(6):
            for node_id in xrange(2):
                if not status[node_id]:
                    resp = mg_conns[node_id].get_node_values(bn)
                    if len(resp.keys()):
                        status[node_id] = True

            if status[0] and status[1]:
                success = True
                break
            else:
                time.sleep(5)

        return success
